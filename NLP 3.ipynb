{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tykggm2Bwmp",
        "outputId": "5f90b5df-b589-4487-e320-b14e0496ac22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize as s, word_tokenize as w, TreebankWordTokenizer as T, WordPunctTokenizer as P, PunktSentenceTokenizer as p\n",
        "from nltk.stem import PorterStemmer as P1, SnowballStemmer as S1, LancasterStemmer as L1\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Hello everyone. Welcome to the Class. You are studying NLP.\"\n",
        "\n",
        "# sent_tokenize\n",
        "sent_tokens = s(txt)\n",
        "print(\"Sent Tokenize:\", sent_tokens)\n",
        "\n",
        "# PunktSentenceTokenizer\n",
        "punkt = p()\n",
        "punkt_tokens = punkt.tokenize(txt)\n",
        "print(\"Punkt Sentence Tokenizer:\", punkt_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_YqgG0ICSoz",
        "outputId": "c0a57fa6-6755-46d5-acc0-0d5e7e899a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sent Tokenize: ['Hello everyone.', 'Welcome to the Class.', 'You are studying NLP.']\n",
            "Punkt Sentence Tokenizer: ['Hello everyone.', 'Welcome to the Class.', 'You are studying NLP.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hello everyone. Welcome to Class.\"\n",
        "\n",
        "#word_tokenize\n",
        "w_tokens = w(sentence)\n",
        "print(\"word_tokenize:\", w_tokens)\n",
        "\n",
        "# TreebankWordTokenizer\n",
        "t_tokens = T().tokenize(sentence)\n",
        "print(\"TreebankWordTokenizer:\", t_tokens)\n",
        "\n",
        "# WordPunctTokenizer\n",
        "p_tokens = P().tokenize(sentence)\n",
        "print(\"WordPunctTokenizer:\", p_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc__yc_oCWQP",
        "outputId": "2880ed5e-09ea-4150-f7f4-9b39826c3e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_tokenize: ['Hello', 'everyone', '.', 'Welcome', 'to', 'Class', '.']\n",
            "TreebankWordTokenizer: ['Hello', 'everyone.', 'Welcome', 'to', 'Class', '.']\n",
            "WordPunctTokenizer: ['Hello', 'everyone', '.', 'Welcome', 'to', 'Class', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words1 = [\"running\", \"jumps\", \"happily\", \"running\", \"happily\"]\n",
        "words2 = ['running', 'jumped', 'happily', 'quickly', 'foxes']\n",
        "\n",
        "# PorterStemmer\n",
        "ps = P1()\n",
        "porter_stems = [ps.stem(word) for word in words1]\n",
        "print(\"PorterStemmer:\", porter_stems)\n",
        "\n",
        "# SnowballStemmer\n",
        "ss = S1(\"english\")\n",
        "snowball_stems = [ss.stem(word) for word in words2]\n",
        "print(\"SnowballStemmer:\", snowball_stems)\n",
        "\n",
        "# LancasterStemmer\n",
        "ls = L1()\n",
        "lancaster_stems = [ls.stem(word) for word in words2]\n",
        "print(\"LancasterStemmer:\", lancaster_stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPK_3sZoCaM2",
        "outputId": "1060361a-9435-4665-e06e-3f6e9fd79fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PorterStemmer: ['run', 'jump', 'happili', 'run', 'happili']\n",
            "SnowballStemmer: ['run', 'jump', 'happili', 'quick', 'fox']\n",
            "LancasterStemmer: ['run', 'jump', 'happy', 'quick', 'fox']\n"
          ]
        }
      ]
    }
  ]
}